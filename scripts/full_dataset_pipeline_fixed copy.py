#!/usr/bin/env python3
"""
åŸºäºå·¥ä½œæ­£å¸¸çš„10å…¬å¸ç‰ˆæœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†å¤„ç†å™¨
===========================================
å¤åˆ¶test_10_companies_optimized_full.pyçš„æˆåŠŸæ¶æ„ï¼Œæ‰©å±•åˆ°5341å…¬å¸
ä¿æŒç›¸åŒçš„NER/REå¤„ç†é€»è¾‘ï¼Œç¡®ä¿æ•°æ®è´¨é‡
"""

import json
import logging
import sys
import time
import os
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple, Any
import gc
import psutil
from datetime import datetime

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))

from config.settings import Settings
from src.components.knowledge_graph_constructor import KnowledgeGraphConstructor
from src.components.embedding_service import EmbeddingService
from src.components.vector_storage import VectorStorage

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('full_dataset_pipeline_fixed.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class FixedFullDatasetProcessor(KnowledgeGraphConstructor):
    """
    åŸºäºæˆåŠŸçš„10å…¬å¸ç‰ˆæœ¬çš„å¤§è§„æ¨¡å¤„ç†å™¨
    ä½¿ç”¨ç›¸åŒçš„NER/REå¤„ç†é€»è¾‘ï¼Œç¡®ä¿æ•°æ®è´¨é‡ä¸€è‡´æ€§
    """
    
    def __init__(
        self,
        max_workers: int = 20,
        batch_size: int = 100,
        enable_embeddings: bool = True,
        embedding_batch_size: int = 8,
        checkpoint_interval: int = 5,
        memory_limit_gb: float = 32.0
    ):
        """
        åˆå§‹åŒ–å¤„ç†å™¨ï¼Œä½¿ç”¨ä¸10å…¬å¸æµ‹è¯•ç›¸åŒçš„æ¶æ„
        """
        self.max_workers = max_workers
        self.batch_size = batch_size
        self.enable_embeddings = enable_embeddings
        self.embedding_batch_size = embedding_batch_size
        self.checkpoint_interval = checkpoint_interval
        self.memory_limit_gb = memory_limit_gb
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_documents': 0,
            'processed_documents': 0,
            'total_entities': 0,
            'total_relations': 0,
            'processing_time': 0,
            'start_time': None,
            'batches_completed': 0,
            'errors': []
        }
        
        # è¾“å‡ºè·¯å¾„
        self.output_dir = Path("output/full_dataset")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶
        self.checkpoint_file = self.output_dir / "processing_checkpoint.json"
        self.progress_file = self.output_dir / "processing_progress.json"
        
        # åµŒå…¥æ¨¡å‹åŠ è½½çŠ¶æ€
        self.embedding_model_loaded = False
        
        logger.info(f"ğŸš€ FixedFullDatasetProcessor initialized:")
        logger.info(f"   â€¢ Max workers: {max_workers}")
        logger.info(f"   â€¢ Batch size: {batch_size}")
        logger.info(f"   â€¢ Embeddings: {'âœ…' if enable_embeddings else 'âŒ'}")
        logger.info(f"   â€¢ Checkpoint interval: {checkpoint_interval}")
        logger.info(f"   â€¢ Memory limit: {memory_limit_gb} GB")
    
    def initialize_components(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ - åŸºäº10å…¬å¸ç‰ˆæœ¬çš„æˆåŠŸæ¨¡å¼"""
        try:
            logger.info("ğŸ”„ Initializing components...")
            
            # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            if self.enable_embeddings:
                try:
                    self.embedding_service = EmbeddingService(
                        batch_size=self.embedding_batch_size
                    )
                    logger.info("âœ… Embedding service initialized")
                    
                    # ç«‹å³åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
                    logger.info("ğŸ”„ Loading embedding model (one-time initialization)...")
                    model_load_start = time.time()
                    if self.embedding_service.load_model():
                        self.embedding_model_loaded = True
                        model_load_time = time.time() - model_load_start
                        logger.info(f"âœ… Embedding model loaded successfully in {model_load_time:.2f}s")
                    else:
                        logger.error("âŒ Failed to load embedding model")
                        self.enable_embeddings = False
                        
                except Exception as e:
                    logger.error(f"âŒ Failed to initialize embedding service: {e}")
                    self.enable_embeddings = False
                
                # åˆå§‹åŒ–å‘é‡å­˜å‚¨
                if self.enable_embeddings:
                    try:
                        self.vector_storage = VectorStorage(
                            db_path=self.output_dir / "vector_store"
                        )
                        self.vector_storage.connect()
                        logger.info("âœ… Vector storage initialized")
                    except Exception as e:
                        logger.error(f"âŒ Failed to initialize vector storage: {e}")
                        self.enable_embeddings = False
            
            # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±æ„é€ å™¨ï¼ˆä½¿ç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼‰
            super().__init__(
                embedding_service=self.embedding_service if self.enable_embeddings else None,
                vector_storage=self.vector_storage if self.enable_embeddings else None
            )
            logger.info("âœ… Knowledge graph constructor initialized")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Component initialization failed: {e}")
            return False
    
    def process_full_dataset(self, corpus_path: Path) -> bool:
        """å¤„ç†å®Œæ•´æ•°æ®é›† - ä½¿ç”¨æ‰¹æ¬¡åŒ–çš„10å…¬å¸å¤„ç†é€»è¾‘"""
        try:
            logger.info("ğŸš€ FIXED FULL DATASET PROCESSING STARTED")
            logger.info("=" * 60)
            
            # åŠ è½½æ•°æ®
            logger.info(f"ğŸ“‚ Loading corpus from {corpus_path}")
            with open(corpus_path, 'r', encoding='utf-8') as f:
                corpus_data = json.load(f)
            
            self.stats['total_documents'] = len(corpus_data)
            self.stats['start_time'] = time.time()
            
            logger.info(f"ğŸ“Š Total documents: {self.stats['total_documents']}")
            
            # è®¡ç®—æ‰¹æ¬¡ä¿¡æ¯
            total_batches = (self.stats['total_documents'] + self.batch_size - 1) // self.batch_size
            
            logger.info(f"ğŸ“‹ Processing in {total_batches} batches of {self.batch_size} docs each")
            
            # åˆå§‹åŒ–ç»„ä»¶
            if not self.initialize_components():
                logger.error("âŒ Failed to initialize components")
                return False
            
            # æ‰¹æ¬¡å¤„ç†
            for batch_idx in range(total_batches):
                start_idx = batch_idx * self.batch_size
                end_idx = min(start_idx + self.batch_size, self.stats['total_documents'])
                
                # å‡†å¤‡æ‰¹æ¬¡æ–‡æ¡£ï¼ˆä½¿ç”¨ä¸10å…¬å¸æµ‹è¯•ç›¸åŒçš„æ ¼å¼ï¼‰
                batch_docs = []
                for i in range(start_idx, end_idx):
                    doc = corpus_data[i]
                    formatted_doc = {
                        'id': f"company_{i:04d}",
                        'title': doc.get('title', f'Company {i}'),
                        'text': doc.get('text', ''),
                        'idx': doc.get('idx', i)
                    }
                    batch_docs.append(formatted_doc)
                
                logger.info(f"\nğŸ“¦ Processing batch {batch_idx + 1}/{total_batches}: {len(batch_docs)} documents")
                batch_start = time.time()
                
                # ä½¿ç”¨ä¸10å…¬å¸æµ‹è¯•å®Œå…¨ç›¸åŒçš„å¤„ç†é€»è¾‘
                batch_results, batch_graph = self.process_batch_like_10_companies(batch_docs)
                
                batch_time = time.time() - batch_start
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats['processed_documents'] += len(batch_docs)
                self.stats['batches_completed'] += 1
                self.stats['processing_time'] += batch_time
                
                logger.info(f"âœ… Batch {batch_idx + 1} completed in {batch_time:.2f}s")
                logger.info(f"ğŸ“Š Progress: {self.stats['processed_documents']}/{self.stats['total_documents']} "
                          f"({self.stats['processed_documents']/self.stats['total_documents']*100:.1f}%)")
                
                # ä¿å­˜è¿›åº¦
                self.save_progress(batch_results, batch_idx)
                
                # æ£€æŸ¥ç‚¹ä¿å­˜
                if (batch_idx + 1) % self.checkpoint_interval == 0:
                    self.save_checkpoint(batch_idx, self.stats['processed_documents'])
                
                # å†…å­˜æ¸…ç†
                self.cleanup_memory()
            
            # æœ€ç»ˆç»Ÿè®¡
            total_time = time.time() - self.stats['start_time']
            
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ‰ FIXED FULL DATASET PROCESSING COMPLETED!")
            logger.info("=" * 60)
            logger.info(f"â±ï¸  Total time: {total_time/3600:.2f} hours")
            logger.info(f"ğŸ“Š Processing speed: {self.stats['total_documents']/(total_time/3600):.0f} companies/hour")
            logger.info(f"ğŸ“ Results saved in: {self.output_dir}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Fixed full dataset processing failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def process_batch_like_10_companies(self, batch_docs: List[Dict]) -> Tuple[Dict, Any]:
        """
        ä½¿ç”¨KnowledgeGraphConstructorçš„æ ‡å‡†å¤„ç†é€»è¾‘
        è¿™ç¡®ä¿äº†NER/REç»“æœçš„ä¸€è‡´æ€§
        """
        logger.info(f"ğŸš€ Processing {len(batch_docs)} documents using standard logic")
        
        # ä½¿ç”¨çˆ¶ç±»çš„process_documentsæ–¹æ³•ï¼Œè¿™æ˜¯æ ‡å‡†çš„å¤„ç†æµç¨‹
        results, graph = super().process_documents(batch_docs)
        
        return results, graph
    
    
    def save_progress(self, batch_results: Dict, batch_idx: int):
        """ä¿å­˜è¿›åº¦ä¿¡æ¯"""
        progress_data = {
            'current_batch': batch_idx + 1,
            'total_batches': (self.stats['total_documents'] + self.batch_size - 1) // self.batch_size,
            'processed_documents': self.stats['processed_documents'],
            'total_documents': self.stats['total_documents'],
            'processing_time': self.stats['processing_time'],
            'timestamp': datetime.now().isoformat(),
            'memory_usage': self.check_memory_usage()
        }
        
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    def save_checkpoint(self, batch_idx: int, processed_docs: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'batch_idx': batch_idx,
            'processed_docs': processed_docs,
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats
        }
        
        with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ Checkpoint saved: batch {batch_idx}, processed {processed_docs}")
    
    def check_memory_usage(self) -> Dict[str, float]:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        memory_gb = memory_info.rss / (1024 ** 3)
        
        return {
            'memory_gb': memory_gb,
            'memory_percent': process.memory_percent(),
            'available_gb': psutil.virtual_memory().available / (1024 ** 3)
        }
    
    def cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        gc.collect()


def main():
    """ä¸»å‡½æ•°"""
    try:
        # é…ç½® - åŸºäº10å…¬å¸æµ‹è¯•çš„æˆåŠŸå‚æ•°
        config = {
            'max_workers': 20,              # ä¸10å…¬å¸æµ‹è¯•ç±»ä¼¼çš„å¹¶è¡Œåº¦
            'batch_size': 50,               # è¾ƒå°æ‰¹æ¬¡ç¡®ä¿ç¨³å®šæ€§
            'enable_embeddings': True,
            'embedding_batch_size': 4,      # ä¿å®ˆçš„åµŒå…¥æ‰¹æ¬¡
            'checkpoint_interval': 2,       # é¢‘ç¹æ£€æŸ¥ç‚¹
            'memory_limit_gb': 32.0
        }
        
        # åŠ è½½æ•°æ®
        project_root = Path(__file__).parent.parent
        corpus_path = project_root / "data" / "corpus.json"
        
        logger.info("ğŸ“‚ Loading documents...")
        if not corpus_path.exists():
            logger.error(f"âŒ Corpus file not found: {corpus_path}")
            return 1
        
        # åˆ›å»ºä¿®å¤ç‰ˆå¤„ç†å™¨
        processor = FixedFullDatasetProcessor(**config)
        
        # è¿è¡Œå¤„ç†
        success = processor.process_full_dataset(corpus_path)
        
        return 0 if success else 1
        
    except Exception as e:
        logger.error(f"âŒ Main execution failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)