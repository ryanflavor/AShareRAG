import functools
import hashlib
import json
import logging
import os
import sqlite3
import time
from pathlib import Path
from string import Template

import httpx
import yaml
from filelock import FileLock
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

from config.settings import Settings
from src.adapters.llm_adapter import LLMAdapter, LLMResponse

logger = logging.getLogger(__name__)


def cache_response(func):
    """ÁºìÂ≠òË£ÖÈ•∞Âô® - Âü∫‰∫éSQLiteÁöÑÈ´òÊÄßËÉΩÁºìÂ≠ò"""

    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        # Â¶ÇÊûúÊú™ÂêØÁî®ÁºìÂ≠ò, Áõ¥Êé•Ë∞ÉÁî®ÂéüÂáΩÊï∞
        if not getattr(self, "enable_cache", False) or not hasattr(
            self, "cache_file_name"
        ):
            return func(self, *args, **kwargs)

        # ÊûÑÂª∫ÁºìÂ≠òÈîÆ
        key_data = {
            "func_name": func.__name__,
            "args": args,
            "kwargs": {k: v for k, v in kwargs.items() if k not in ["self"]},
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        key_hash = hashlib.sha256(key_str.encode("utf-8")).hexdigest()

        # ÁºìÂ≠òÊñá‰ª∂ÂíåÈîÅ
        cache_file = self.cache_file_name
        lock_file = cache_file + ".lock"

        # Â∞ùËØï‰ªéÁºìÂ≠òËØªÂèñ
        with FileLock(lock_file):
            try:
                conn = sqlite3.connect(cache_file)
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS cache (
                        key TEXT PRIMARY KEY,
                        result TEXT,
                        timestamp REAL
                    )
                """)
                conn.commit()

                c.execute("SELECT result FROM cache WHERE key = ?", (key_hash,))
                row = c.fetchone()
                conn.close()

                if row is not None:
                    logger.info(f"üéØ Cache hit for {func.__name__}")
                    cached_data = json.loads(row[0])
                    
                    # If this is a generate method, reconstruct LLMResponse object
                    if func.__name__ == "generate":
                        from src.adapters.llm_adapter import LLMResponse
                        
                        if isinstance(cached_data, str):
                            # Old cache format: just the content string
                            return LLMResponse(
                                content=cached_data,
                                model=self.settings.deepseek_model,
                                usage={}
                            )
                        elif isinstance(cached_data, dict):
                            # New cache format: full object
                            return LLMResponse(
                                content=cached_data.get("content", ""),
                                model=cached_data.get("model", ""),
                                usage=cached_data.get("usage", {})
                            )
                    
                    return cached_data
            except Exception as e:
                logger.warning(f"Cache read error: {e}")

        # ÁºìÂ≠òÊú™ÂëΩ‰∏≠, Ë∞ÉÁî®ÂéüÂáΩÊï∞
        logger.info(f"üì° Cache miss, calling API for {func.__name__}")
        result = func(self, *args, **kwargs)

        # Â≠òÂÇ®Âà∞ÁºìÂ≠ò
        with FileLock(lock_file):
            try:
                conn = sqlite3.connect(cache_file)
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS cache (
                        key TEXT PRIMARY KEY,
                        result TEXT,
                        timestamp REAL
                    )
                """)
                result_str = json.dumps(result, default=str, ensure_ascii=False)
                c.execute(
                    "INSERT OR REPLACE INTO cache (key, result, timestamp) "
                    "VALUES (?, ?, ?)",
                    (key_hash, result_str, time.time()),
                )
                conn.commit()
                conn.close()
                logger.info(f"üíæ Cached result for {func.__name__}")
            except Exception as e:
                logger.warning(f"Cache write error: {e}")

        return result

    return wrapper


class DeepSeekAdapter(LLMAdapter):
    """DeepSeek‰∏ìÁî®È´òÊÄßËÉΩLLMÈÄÇÈÖçÂô® - ‰ºòÂåñÁΩëÁªúÂíåÁºìÂ≠òÊÄßËÉΩ"""

    def __init__(self, enable_cache: bool = True, high_throughput: bool = True):
        """
        ÂàùÂßãÂåñÈ´òÊÄßËÉΩLLMÈÄÇÈÖçÂô®

        Args:
            enable_cache: ÊòØÂê¶ÂêØÁî®Êú¨Âú∞ÁºìÂ≠ò
            high_throughput: ÊòØÂê¶‰ΩøÁî®È´òÂêûÂêêÈáèHTTPÈÖçÁΩÆ
        """
        self.settings = Settings()
        self.enable_cache = enable_cache

        # ËÆæÁΩÆÁºìÂ≠ò
        if enable_cache:
            cache_dir = os.path.join(os.getcwd(), ".cache", "llm")
            os.makedirs(cache_dir, exist_ok=True)
            self.cache_file_name = os.path.join(cache_dir, "llm_cache.sqlite")

        # ÂàùÂßãÂåñÈ´òÊÄßËÉΩHTTPÂÆ¢Êà∑Á´Ø
        http_client = None
        if high_throughput:
            logger.info("üöÄ Initializing high-performance HTTP client...")
            limits = httpx.Limits(max_connections=500, max_keepalive_connections=100)
            http_client = httpx.Client(
                limits=limits,
                timeout=httpx.Timeout(300.0, read=300.0),  # 5ÂàÜÈíüË∂ÖÊó∂
            )
            logger.info(
                "‚úÖ HTTP client configured with 500 connections, 100 keep-alive"
            )

        # ÂàùÂßãÂåñOpenAIÂÆ¢Êà∑Á´Ø
        self.client = OpenAI(
            api_key=self.settings.deepseek_api_key,
            base_url=self.settings.deepseek_api_base,
            http_client=http_client,
            max_retries=0,  # Êàë‰ª¨‰ΩøÁî®tenacityËøõË°åÈáçËØï
        )

        # Âä†ËΩΩÊèêÁ§∫ËØç
        self._load_prompts()

        # Set model name property
        self.model_name = self.settings.deepseek_model

        logger.info(
            "‚ö° DeepSeekAdapter initialized with high-performance configuration"
        )

    def _load_prompts(self):
        """Âä†ËΩΩÊèêÁ§∫ËØçÈÖçÁΩÆ"""
        prompts_path = Path(self.settings.prompts_path)
        if not prompts_path.exists():
            raise FileNotFoundError(f"Prompts file not found: {prompts_path}")

        with open(prompts_path, encoding="utf-8") as f:
            self.prompts = yaml.safe_load(f)

        if "ner" not in self.prompts:
            raise ValueError("NER prompts not found in configuration")

        self.ner_config = self.prompts["ner"]
        self.re_config = self.prompts.get("re", None)

    @retry(
        stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10)
    )
    def _call_api_with_retry(self, messages: list[dict], operation: str) -> str:
        """‰ΩøÁî®tenacityËøõË°åAPIË∞ÉÁî®ÈáçËØï"""
        logger.info(f"üåê {operation} API call starting...")

        start_time = time.time()
        try:
            response = self.client.chat.completions.create(
                model=self.settings.deepseek_model,
                messages=messages,
                temperature=0.0,
                max_tokens=4000,
            )

            api_time = time.time() - start_time
            content = response.choices[0].message.content

            logger.info(f"‚úÖ {operation} API success in {api_time:.3f}s")
            logger.debug(
                f"üìä Tokens - Input: {response.usage.prompt_tokens}, "
                f"Output: {response.usage.completion_tokens}"
            )

            return content

        except Exception as e:
            api_time = time.time() - start_time
            logger.error(f"‚ùå {operation} API failed after {api_time:.3f}s: {e}")
            raise

    @cache_response
    def extract_entities(
        self, text: str, include_types: bool = True
    ) -> list[dict] | list[str]:
        """
        ÊèêÂèñÂëΩÂêçÂÆû‰Ωì(Â∏¶È´òÊÄßËÉΩÁºìÂ≠ò)

        Args:
            text: ËæìÂÖ•ÊñáÊú¨
            include_types: ÊòØÂê¶ËøîÂõûÁ±ªÂûãÂåñÂÆû‰Ωì

        Returns:
            ÂÆû‰ΩìÂàóË°®
        """
        if not text or not text.strip():
            return []

        try:
            # ÊûÑÂª∫Ê∂àÊÅØ
            messages = self._build_ner_messages(text)

            # APIË∞ÉÁî®
            response_content = self._call_api_with_retry(messages, "NER")

            # Ëß£ÊûêÂìçÂ∫î
            return self._parse_ner_response(response_content, include_types)

        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            return []

    def _build_ner_messages(self, text: str) -> list[dict]:
        """ÊûÑÂª∫NERÊ∂àÊÅØ"""
        messages = []

        # Á≥ªÁªüÊèêÁ§∫
        messages.append({"role": "system", "content": self.ner_config["system"]})

        # Á§∫‰æã
        if self.ner_config.get("examples"):
            for example in self.ner_config["examples"]:
                messages.append({"role": "user", "content": example["user"]})
                messages.append({"role": "assistant", "content": example["assistant"]})

        # Áî®Êà∑Êü•ËØ¢
        template = Template(self.ner_config["template"])
        user_content = template.substitute(passage=text)
        messages.append({"role": "user", "content": user_content})

        return messages

    def _parse_ner_response(
        self, response: str, include_types: bool
    ) -> list[dict] | list[str]:
        """Ëß£ÊûêNERÂìçÂ∫î"""
        try:
            # Ê∏ÖÁêÜÂìçÂ∫î
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]

            data = json.loads(response)

            if isinstance(data, dict) and "named_entities" in data:
                entities = data["named_entities"]
                if not isinstance(entities, list):
                    return []

                if not entities:
                    return []

                # Â§ÑÁêÜÁ±ªÂûãÂåñÂÆû‰Ωì
                if include_types and isinstance(entities[0], dict):
                    valid_entities = []
                    for entity in entities:
                        if (
                            isinstance(entity, dict)
                            and "text" in entity
                            and "type" in entity
                            and entity["text"]
                            and entity["text"].strip()
                        ):
                            valid_entities.append(
                                {
                                    "text": str(entity["text"]).strip(),
                                    "type": entity["type"],
                                }
                            )
                    return valid_entities
                else:
                    # ËøîÂõûÂ≠óÁ¨¶‰∏≤ÂàóË°®
                    return [str(e).strip() for e in entities if e and str(e).strip()]

            return []

        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Failed to parse NER response: {e}")
            return []

    @cache_response
    def extract_relations(
        self, text: str, entities: list[dict[str, str]]
    ) -> list[list[str]]:
        """
        ÊèêÂèñÂÖ≥Á≥ª(Â∏¶È´òÊÄßËÉΩÁºìÂ≠ò)

        Args:
            text: ËæìÂÖ•ÊñáÊú¨
            entities: ÂÆû‰ΩìÂàóË°®

        Returns:
            ÂÖ≥Á≥ª‰∏âÂÖÉÁªÑÂàóË°®
        """
        if not text or not text.strip() or not entities:
            return []

        if not self.re_config:
            logger.error("RE configuration not found")
            return []

        try:
            # ÊèêÂèñÂÆû‰ΩìÊñáÊú¨
            entity_texts = [entity["text"] for entity in entities if "text" in entity]
            if not entity_texts:
                return []

            # ÊûÑÂª∫Ê∂àÊÅØ
            messages = self._build_re_messages(text, entity_texts)

            # APIË∞ÉÁî®
            response_content = self._call_api_with_retry(messages, "RE")

            # Ëß£ÊûêÂìçÂ∫î
            return self._parse_re_response(response_content, entity_texts)

        except Exception as e:
            logger.error(f"Relation extraction failed: {e}")
            return []

    def _build_re_messages(self, text: str, entity_texts: list[str]) -> list[dict]:
        """ÊûÑÂª∫REÊ∂àÊÅØ"""
        messages = []

        # Á≥ªÁªüÊèêÁ§∫
        messages.append({"role": "system", "content": self.re_config["system"]})

        # Á§∫‰æã
        if self.re_config.get("examples"):
            for example in self.re_config["examples"]:
                messages.append({"role": "user", "content": example["user"]})
                messages.append({"role": "assistant", "content": example["assistant"]})

        # Ê†ºÂºèÂåñÂÆû‰ΩìJSON
        entity_json = json.dumps({"named_entities": entity_texts}, ensure_ascii=False)

        # Áî®Êà∑Êü•ËØ¢
        template = Template(self.re_config["template"])
        user_content = template.substitute(passage=text, named_entity_json=entity_json)
        messages.append({"role": "user", "content": user_content})

        return messages

    def _parse_re_response(
        self, response: str, entity_texts: list[str]
    ) -> list[list[str]]:
        """Ëß£ÊûêREÂìçÂ∫î"""
        try:
            # Ê∏ÖÁêÜÂìçÂ∫î
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]

            data = json.loads(response)

            if isinstance(data, dict) and "triples" in data:
                raw_triples = data["triples"]
                if not isinstance(raw_triples, list):
                    return []

                # È™åËØÅÂíåÂéªÈáç
                entity_set = set(entity_texts)
                valid_triples = []
                seen_triples = set()

                for triple in raw_triples:
                    if isinstance(triple, list) and len(triple) == 3:
                        triple = [str(elem).strip() for elem in triple]

                        # Ëá≥Â∞ëÂåÖÂê´‰∏Ä‰∏™ÂëΩÂêçÂÆû‰Ωì
                        if any(elem in entity_set for elem in triple):
                            triple_tuple = tuple(triple)
                            if triple_tuple not in seen_triples:
                                seen_triples.add(triple_tuple)
                                valid_triples.append(triple)

                return valid_triples

            return []

        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Failed to parse RE response: {e}")
            return []

    def clear_cache(self):
        """Ê∏ÖÈô§ÁºìÂ≠ò"""
        if self.enable_cache and hasattr(self, "cache_file_name"):
            if os.path.exists(self.cache_file_name):
                os.remove(self.cache_file_name)
                logger.info("üóëÔ∏è Cache cleared")

    def get_cache_stats(self) -> dict:
        """Ëé∑ÂèñÁºìÂ≠òÁªüËÆ°"""
        if not self.enable_cache or not hasattr(self, "cache_file_name"):
            return {"cache_enabled": False}

        if not os.path.exists(self.cache_file_name):
            return {"cache_enabled": True, "entries": 0}

        try:
            conn = sqlite3.connect(self.cache_file_name)
            c = conn.cursor()
            c.execute("SELECT COUNT(*) FROM cache")
            count = c.fetchone()[0]
            conn.close()
            return {"cache_enabled": True, "entries": count}
        except Exception:
            return {"cache_enabled": True, "entries": "unknown"}

    def get_http_stats(self) -> dict:
        """Ëé∑ÂèñHTTPÂÆ¢Êà∑Á´ØÁªüËÆ°‰ø°ÊÅØ"""
        http_client = getattr(self.client, "_client", None)
        if http_client and hasattr(http_client, "_pool"):
            pool = http_client._pool
            return {
                "max_connections": getattr(pool, "_max_connections", None),
                "max_keepalive": getattr(pool, "_max_keepalive_connections", None),
                "active_connections": len(getattr(pool, "_connections", [])),
            }
        return {"http_stats": "Not available"}

    @cache_response
    def generate(
        self,
        prompt: str,
        max_tokens: int = 5000,
        temperature: float = 0.1,
        top_p: float = 0.9,
        **kwargs,
    ) -> LLMResponse:
        """
        Generate text completion using DeepSeek.

        Args:
            prompt: The prompt to generate from
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            **kwargs: Additional parameters

        Returns:
            LLMResponse with generated text
        """
        try:
            messages = [{"role": "user", "content": prompt}]

            # Add system message if provided
            if "system_prompt" in kwargs:
                messages.insert(
                    0, {"role": "system", "content": kwargs["system_prompt"]}
                )

            logger.info("üåê Generation API call starting...")
            start_time = time.time()

            response = self.client.chat.completions.create(
                model=self.settings.deepseek_model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
            )

            api_time = time.time() - start_time
            content = response.choices[0].message.content

            logger.info(f"‚úÖ Generation API success in {api_time:.3f}s")
            logger.debug(
                f"üìä Tokens - Input: {response.usage.prompt_tokens}, "
                f"Output: {response.usage.completion_tokens}"
            )

            return LLMResponse(
                content=content,
                model=self.settings.deepseek_model,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                },
            )

        except Exception as e:
            logger.error(f"‚ùå Generation API failed: {e}")
            raise
