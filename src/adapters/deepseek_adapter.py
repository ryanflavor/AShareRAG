import functools
import hashlib
import json
import logging
import os
import sqlite3
import time
from pathlib import Path
from string import Template

import httpx
import yaml
from filelock import FileLock
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

from config.settings import Settings
from src.adapters.llm_adapter import LLMAdapter

logger = logging.getLogger(__name__)


def cache_response(func):
    """ç¼“å­˜è£…é¥°å™¨ - åŸºäºSQLiteçš„é«˜æ€§èƒ½ç¼“å­˜"""

    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        # å¦‚æœæœªå¯ç”¨ç¼“å­˜, ç›´æ¥è°ƒç”¨åŸå‡½æ•°
        if not getattr(self, "enable_cache", False) or not hasattr(
            self, "cache_file_name"
        ):
            return func(self, *args, **kwargs)

        # æ„å»ºç¼“å­˜é”®
        key_data = {
            "func_name": func.__name__,
            "args": args,
            "kwargs": {k: v for k, v in kwargs.items() if k not in ["self"]},
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        key_hash = hashlib.sha256(key_str.encode("utf-8")).hexdigest()

        # ç¼“å­˜æ–‡ä»¶å’Œé”
        cache_file = self.cache_file_name
        lock_file = cache_file + ".lock"

        # å°è¯•ä»ç¼“å­˜è¯»å–
        with FileLock(lock_file):
            try:
                conn = sqlite3.connect(cache_file)
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS cache (
                        key TEXT PRIMARY KEY,
                        result TEXT,
                        timestamp REAL
                    )
                """)
                conn.commit()

                c.execute("SELECT result FROM cache WHERE key = ?", (key_hash,))
                row = c.fetchone()
                conn.close()

                if row is not None:
                    logger.info(f"ğŸ¯ Cache hit for {func.__name__}")
                    return json.loads(row[0])
            except Exception as e:
                logger.warning(f"Cache read error: {e}")

        # ç¼“å­˜æœªå‘½ä¸­, è°ƒç”¨åŸå‡½æ•°
        logger.info(f"ğŸ“¡ Cache miss, calling API for {func.__name__}")
        result = func(self, *args, **kwargs)

        # å­˜å‚¨åˆ°ç¼“å­˜
        with FileLock(lock_file):
            try:
                conn = sqlite3.connect(cache_file)
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS cache (
                        key TEXT PRIMARY KEY,
                        result TEXT,
                        timestamp REAL
                    )
                """)
                result_str = json.dumps(result, default=str, ensure_ascii=False)
                c.execute(
                    "INSERT OR REPLACE INTO cache (key, result, timestamp) VALUES (?, ?, ?)",
                    (key_hash, result_str, time.time()),
                )
                conn.commit()
                conn.close()
                logger.info(f"ğŸ’¾ Cached result for {func.__name__}")
            except Exception as e:
                logger.warning(f"Cache write error: {e}")

        return result

    return wrapper


class DeepSeekAdapter(LLMAdapter):
    """DeepSeekä¸“ç”¨é«˜æ€§èƒ½LLMé€‚é…å™¨ - ä¼˜åŒ–ç½‘ç»œå’Œç¼“å­˜æ€§èƒ½"""

    def __init__(self, enable_cache: bool = True, high_throughput: bool = True):
        """
        åˆå§‹åŒ–é«˜æ€§èƒ½LLMé€‚é…å™¨

        Args:
            enable_cache: æ˜¯å¦å¯ç”¨æœ¬åœ°ç¼“å­˜
            high_throughput: æ˜¯å¦ä½¿ç”¨é«˜ååé‡HTTPé…ç½®
        """
        self.settings = Settings()
        self.enable_cache = enable_cache

        # è®¾ç½®ç¼“å­˜
        if enable_cache:
            cache_dir = os.path.join(os.getcwd(), ".cache", "llm")
            os.makedirs(cache_dir, exist_ok=True)
            self.cache_file_name = os.path.join(cache_dir, "llm_cache.sqlite")

        # åˆå§‹åŒ–é«˜æ€§èƒ½HTTPå®¢æˆ·ç«¯
        http_client = None
        if high_throughput:
            logger.info("ğŸš€ Initializing high-performance HTTP client...")
            limits = httpx.Limits(max_connections=500, max_keepalive_connections=100)
            http_client = httpx.Client(
                limits=limits,
                timeout=httpx.Timeout(300.0, read=300.0),  # 5åˆ†é’Ÿè¶…æ—¶
            )
            logger.info(
                "âœ… HTTP client configured with 500 connections, 100 keep-alive"
            )

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.settings.deepseek_api_key,
            base_url=self.settings.deepseek_api_base,
            http_client=http_client,
            max_retries=0,  # æˆ‘ä»¬ä½¿ç”¨tenacityè¿›è¡Œé‡è¯•
        )

        # åŠ è½½æç¤ºè¯
        self._load_prompts()

        logger.info(
            "âš¡ DeepSeekAdapter initialized with high-performance configuration"
        )

    def _load_prompts(self):
        """åŠ è½½æç¤ºè¯é…ç½®"""
        prompts_path = Path(self.settings.prompts_path)
        if not prompts_path.exists():
            raise FileNotFoundError(f"Prompts file not found: {prompts_path}")

        with open(prompts_path, encoding="utf-8") as f:
            self.prompts = yaml.safe_load(f)

        if "ner" not in self.prompts:
            raise ValueError("NER prompts not found in configuration")

        self.ner_config = self.prompts["ner"]
        self.re_config = self.prompts.get("re", None)

    @retry(
        stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10)
    )
    def _call_api_with_retry(self, messages: list[dict], operation: str) -> str:
        """ä½¿ç”¨tenacityè¿›è¡ŒAPIè°ƒç”¨é‡è¯•"""
        logger.info(f"ğŸŒ {operation} API call starting...")

        start_time = time.time()
        try:
            response = self.client.chat.completions.create(
                model=self.settings.deepseek_model,
                messages=messages,
                temperature=0.0,
                max_tokens=4000,
            )

            api_time = time.time() - start_time
            content = response.choices[0].message.content

            logger.info(f"âœ… {operation} API success in {api_time:.3f}s")
            logger.debug(
                f"ğŸ“Š Tokens - Input: {response.usage.prompt_tokens}, Output: {response.usage.completion_tokens}"
            )

            return content

        except Exception as e:
            api_time = time.time() - start_time
            logger.error(f"âŒ {operation} API failed after {api_time:.3f}s: {e}")
            raise

    @cache_response
    def extract_entities(
        self, text: str, include_types: bool = True
    ) -> list[dict] | list[str]:
        """
        æå–å‘½åå®ä½“(å¸¦é«˜æ€§èƒ½ç¼“å­˜)

        Args:
            text: è¾“å…¥æ–‡æœ¬
            include_types: æ˜¯å¦è¿”å›ç±»å‹åŒ–å®ä½“

        Returns:
            å®ä½“åˆ—è¡¨
        """
        if not text or not text.strip():
            return []

        try:
            # æ„å»ºæ¶ˆæ¯
            messages = self._build_ner_messages(text)

            # APIè°ƒç”¨
            response_content = self._call_api_with_retry(messages, "NER")

            # è§£æå“åº”
            return self._parse_ner_response(response_content, include_types)

        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            return []

    def _build_ner_messages(self, text: str) -> list[dict]:
        """æ„å»ºNERæ¶ˆæ¯"""
        messages = []

        # ç³»ç»Ÿæç¤º
        messages.append({"role": "system", "content": self.ner_config["system"]})

        # ç¤ºä¾‹
        if self.ner_config.get("examples"):
            for example in self.ner_config["examples"]:
                messages.append({"role": "user", "content": example["user"]})
                messages.append({"role": "assistant", "content": example["assistant"]})

        # ç”¨æˆ·æŸ¥è¯¢
        template = Template(self.ner_config["template"])
        user_content = template.substitute(passage=text)
        messages.append({"role": "user", "content": user_content})

        return messages

    def _parse_ner_response(
        self, response: str, include_types: bool
    ) -> list[dict] | list[str]:
        """è§£æNERå“åº”"""
        try:
            # æ¸…ç†å“åº”
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]

            data = json.loads(response)

            if isinstance(data, dict) and "named_entities" in data:
                entities = data["named_entities"]
                if not isinstance(entities, list):
                    return []

                if not entities:
                    return []

                # å¤„ç†ç±»å‹åŒ–å®ä½“
                if include_types and isinstance(entities[0], dict):
                    valid_entities = []
                    for entity in entities:
                        if (
                            isinstance(entity, dict)
                            and "text" in entity
                            and "type" in entity
                            and entity["text"]
                            and entity["text"].strip()
                        ):
                            valid_entities.append(
                                {
                                    "text": str(entity["text"]).strip(),
                                    "type": entity["type"],
                                }
                            )
                    return valid_entities
                else:
                    # è¿”å›å­—ç¬¦ä¸²åˆ—è¡¨
                    return [str(e).strip() for e in entities if e and str(e).strip()]

            return []

        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Failed to parse NER response: {e}")
            return []

    @cache_response
    def extract_relations(
        self, text: str, entities: list[dict[str, str]]
    ) -> list[list[str]]:
        """
        æå–å…³ç³»(å¸¦é«˜æ€§èƒ½ç¼“å­˜)

        Args:
            text: è¾“å…¥æ–‡æœ¬
            entities: å®ä½“åˆ—è¡¨

        Returns:
            å…³ç³»ä¸‰å…ƒç»„åˆ—è¡¨
        """
        if not text or not text.strip() or not entities:
            return []

        if not self.re_config:
            logger.error("RE configuration not found")
            return []

        try:
            # æå–å®ä½“æ–‡æœ¬
            entity_texts = [entity["text"] for entity in entities if "text" in entity]
            if not entity_texts:
                return []

            # æ„å»ºæ¶ˆæ¯
            messages = self._build_re_messages(text, entity_texts)

            # APIè°ƒç”¨
            response_content = self._call_api_with_retry(messages, "RE")

            # è§£æå“åº”
            return self._parse_re_response(response_content, entity_texts)

        except Exception as e:
            logger.error(f"Relation extraction failed: {e}")
            return []

    def _build_re_messages(self, text: str, entity_texts: list[str]) -> list[dict]:
        """æ„å»ºREæ¶ˆæ¯"""
        messages = []

        # ç³»ç»Ÿæç¤º
        messages.append({"role": "system", "content": self.re_config["system"]})

        # ç¤ºä¾‹
        if self.re_config.get("examples"):
            for example in self.re_config["examples"]:
                messages.append({"role": "user", "content": example["user"]})
                messages.append({"role": "assistant", "content": example["assistant"]})

        # æ ¼å¼åŒ–å®ä½“JSON
        entity_json = json.dumps({"named_entities": entity_texts}, ensure_ascii=False)

        # ç”¨æˆ·æŸ¥è¯¢
        template = Template(self.re_config["template"])
        user_content = template.substitute(passage=text, named_entity_json=entity_json)
        messages.append({"role": "user", "content": user_content})

        return messages

    def _parse_re_response(
        self, response: str, entity_texts: list[str]
    ) -> list[list[str]]:
        """è§£æREå“åº”"""
        try:
            # æ¸…ç†å“åº”
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]

            data = json.loads(response)

            if isinstance(data, dict) and "triples" in data:
                raw_triples = data["triples"]
                if not isinstance(raw_triples, list):
                    return []

                # éªŒè¯å’Œå»é‡
                entity_set = set(entity_texts)
                valid_triples = []
                seen_triples = set()

                for triple in raw_triples:
                    if isinstance(triple, list) and len(triple) == 3:
                        triple = [str(elem).strip() for elem in triple]

                        # è‡³å°‘åŒ…å«ä¸€ä¸ªå‘½åå®ä½“
                        if any(elem in entity_set for elem in triple):
                            triple_tuple = tuple(triple)
                            if triple_tuple not in seen_triples:
                                seen_triples.add(triple_tuple)
                                valid_triples.append(triple)

                return valid_triples

            return []

        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Failed to parse RE response: {e}")
            return []

    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        if self.enable_cache and hasattr(self, "cache_file_name"):
            if os.path.exists(self.cache_file_name):
                os.remove(self.cache_file_name)
                logger.info("ğŸ—‘ï¸ Cache cleared")

    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        if not self.enable_cache or not hasattr(self, "cache_file_name"):
            return {"cache_enabled": False}

        if not os.path.exists(self.cache_file_name):
            return {"cache_enabled": True, "entries": 0}

        try:
            conn = sqlite3.connect(self.cache_file_name)
            c = conn.cursor()
            c.execute("SELECT COUNT(*) FROM cache")
            count = c.fetchone()[0]
            conn.close()
            return {"cache_enabled": True, "entries": count}
        except Exception:
            return {"cache_enabled": True, "entries": "unknown"}

    def get_http_stats(self) -> dict:
        """è·å–HTTPå®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯"""
        http_client = getattr(self.client, "_client", None)
        if http_client and hasattr(http_client, "_pool"):
            pool = http_client._pool
            return {
                "max_connections": getattr(pool, "_max_connections", None),
                "max_keepalive": getattr(pool, "_max_keepalive_connections", None),
                "active_connections": len(getattr(pool, "_connections", [])),
            }
        return {"http_stats": "Not available"}
