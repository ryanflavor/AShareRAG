#!/usr/bin/env python3
"""
Qwen3-Embedding-4B é«˜çº§ä½¿ç”¨ç¤ºä¾‹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¸€ä¸ªä¼˜é›…ä¸”å¥å£®çš„åµŒå…¥æ¨¡å‹ä½¿ç”¨æ¨¡æ¿ï¼Œå±•ç¤ºäº†å†…å­˜ç®¡ç†ã€é”™è¯¯å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–çš„æœ€ä½³å®è·µã€‚

ç‰¹æ€§ï¼š
- ğŸ›¡ï¸  è‡ªåŠ¨å†…å­˜ç®¡ç†å’ŒOOMæ¢å¤
- ğŸ“Š å®æ—¶æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡
- ğŸ”„ æ™ºèƒ½é‡è¯•æœºåˆ¶
- ğŸ¯ ä¼˜åŒ–çš„æ‰¹å¤„ç†ç­–ç•¥
- ğŸ“ˆ è¯¦ç»†çš„æ€§èƒ½åŸºå‡†æµ‹è¯•

ä½œè€…: HippoRAG Team
ç‰ˆæœ¬: 2.0
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import gc
import logging
import os
import sys
import time
from contextlib import contextmanager
from dataclasses import dataclass
from functools import wraps
from typing import Any

import numpy as np
import psutil
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# é…ç½®å’Œæ•°æ®ç±»
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


@dataclass
class MemoryInfo:
    """å†…å­˜ä¿¡æ¯æ•°æ®ç±»"""

    gpu_used: float
    gpu_total: float
    gpu_free: float
    gpu_peak: float
    cpu_percent: float

    def __str__(self):
        return (
            f"GPU: {self.gpu_used:.2f}/{self.gpu_total:.2f}GB "
            f"(ç©ºé—²: {self.gpu_free:.2f}GB, å³°å€¼: {self.gpu_peak:.2f}GB)"
        )


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""

    processing_time: float
    texts_count: int
    embeddings_shape: tuple[int, int]
    throughput: float
    memory_before: MemoryInfo
    memory_after: MemoryInfo

    def __str__(self):
        return (
            f"å¤„ç† {self.texts_count} ä¸ªæ–‡æœ¬ï¼Œè€—æ—¶ {self.processing_time:.2f}ç§’\n"
            f"ååé‡: {self.throughput:.1f} texts/ç§’\n"
            f"åµŒå…¥ç»´åº¦: {self.embeddings_shape[1]}"
        )


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# å†…å­˜ç®¡ç†å·¥å…·
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


def get_memory_info() -> MemoryInfo:
    """
    è·å–è¯¦ç»†çš„å†…å­˜ä½¿ç”¨ä¿¡æ¯

    Returns:
        MemoryInfo: åŒ…å«GPUå’ŒCPUå†…å­˜ä¿¡æ¯çš„æ•°æ®ç±»
    """
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / 1024**3
        gpu_memory_max = torch.cuda.max_memory_allocated() / 1024**3
        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_free = gpu_total - gpu_memory
    else:
        gpu_memory = gpu_memory_max = gpu_total = gpu_free = 0.0

    cpu_percent = psutil.virtual_memory().percent

    return MemoryInfo(
        gpu_used=gpu_memory,
        gpu_total=gpu_total,
        gpu_free=gpu_free,
        gpu_peak=gpu_memory_max,
        cpu_percent=cpu_percent,
    )


def clear_memory():
    """
    æ¸…ç†GPUå’Œç³»ç»Ÿå†…å­˜

    è¿™ä¸ªå‡½æ•°ä¼šï¼š
    1. æ¸…ç©ºCUDAç¼“å­˜
    2. è§¦å‘Pythonåƒåœ¾å›æ”¶
    """
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


def auto_retry_on_oom(
    max_retries: int = 3, wait_time: float = 2.0, backoff_factor: float = 1.5
):
    """
    è£…é¥°å™¨ï¼šè‡ªåŠ¨å¤„ç†CUDA OOMé”™è¯¯

    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        wait_time: åˆå§‹ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        backoff_factor: ç­‰å¾…æ—¶é—´é€’å¢å› å­

    Returns:
        è£…é¥°åçš„å‡½æ•°ï¼Œå…·æœ‰è‡ªåŠ¨é‡è¯•èƒ½åŠ›
    """

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            current_wait = wait_time

            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except torch.cuda.OutOfMemoryError:
                    logging.warning(f"âš ï¸  CUDA OOM (å°è¯• {attempt + 1}/{max_retries})")

                    mem_info = get_memory_info()
                    logging.info(f"å½“å‰å†…å­˜çŠ¶æ€: {mem_info}")

                    if attempt < max_retries - 1:
                        logging.info(f"ğŸ”„ æ¸…ç†å†…å­˜å¹¶ç­‰å¾… {current_wait:.1f} ç§’...")
                        clear_memory()
                        time.sleep(current_wait)
                        current_wait *= backoff_factor

                        # å†æ¬¡æ£€æŸ¥å†…å­˜
                        new_mem_info = get_memory_info()
                        logging.info(f"æ¸…ç†åå†…å­˜: {new_mem_info}")
                    else:
                        logging.error("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ“ä½œå¤±è´¥")
                        return None
                except Exception as e:
                    logging.error(f"âŒ å…¶ä»–é”™è¯¯: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(wait_time)
                    else:
                        raise e
            return None

        return wrapper

    return decorator


@contextmanager
def memory_management(tag: str = ""):
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šè‡ªåŠ¨å†…å­˜ç®¡ç†å’Œæ€§èƒ½è·Ÿè¸ª

    Args:
        tag: æ“ä½œæ ‡ç­¾ï¼Œç”¨äºæ—¥å¿—è®°å½•

    Usage:
        with memory_management("æ¨¡å‹åŠ è½½"):
            model = load_model()
    """
    tag_str = f"[{tag}] " if tag else ""
    initial_memory = get_memory_info()
    logging.info(f"ğŸš€ {tag_str}å¼€å§‹ - å†…å­˜çŠ¶æ€: {initial_memory}")

    start_time = time.time()
    try:
        yield
    finally:
        clear_memory()
        elapsed_time = time.time() - start_time
        final_memory = get_memory_info()
        logging.info(f"âœ… {tag_str}å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’")
        logging.info(f"ğŸ§¹ {tag_str}æ¸…ç†åå†…å­˜: {final_memory}")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# æ¨¡å‹ç®¡ç†
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


class Qwen3EmbeddingManager:
    """
    Qwen3åµŒå…¥æ¨¡å‹ç®¡ç†å™¨

    æä¾›äº†æ¨¡å‹åŠ è½½ã€åµŒå…¥ç”Ÿæˆå’Œæ€§èƒ½ç›‘æ§çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
    """

    def __init__(
        self,
        model_name: str = "Qwen/Qwen3-Embedding-4B",
        device: str | None = None,
        embedding_dim: int | None = None,
    ):
        """
        åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ç®¡ç†å™¨

        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡ ('cuda', 'cpu' æˆ– Noneè‡ªåŠ¨é€‰æ‹©)
            embedding_dim: åµŒå…¥ç»´åº¦ (Noneä½¿ç”¨é»˜è®¤2560)
        """
        self.model_name = model_name
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.embedding_dim = embedding_dim or int(
            os.getenv("QWEN3_EMBEDDING_DIM", "2560")
        )
        self.model = None
        self.performance_history = []

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            datefmt="%H:%M:%S",
        )

    def load_model(self, max_retries: int = 3) -> bool:
        """
        å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œå¸¦è‡ªåŠ¨é‡è¯•å’Œé™çº§æœºåˆ¶

        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        with memory_management("æ¨¡å‹åŠ è½½"):
            for attempt in range(max_retries):
                try:
                    logging.info(f"ğŸ”„ åŠ è½½æ¨¡å‹ (å°è¯• {attempt + 1}/{max_retries})...")

                    # æ„å»ºæ¨¡å‹å‚æ•°
                    model_kwargs = {"trust_remote_code": True, "device": self.device}

                    # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰ç»´åº¦
                    if self.embedding_dim != 2560:
                        model_kwargs["truncate_dim"] = self.embedding_dim

                    self.model = SentenceTransformer(self.model_name, **model_kwargs)

                    # éªŒè¯æ¨¡å‹ç»´åº¦
                    actual_dim = self.model.get_sentence_embedding_dimension()
                    logging.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! åµŒå…¥ç»´åº¦: {actual_dim}")

                    return True

                except torch.cuda.OutOfMemoryError as e:
                    logging.error(f"âš ï¸  GPUå†…å­˜ä¸è¶³: {str(e)[:100]}...")
                    clear_memory()

                    if attempt == max_retries - 1 and self.device == "cuda":
                        logging.warning("ğŸ”„ å°è¯•åˆ‡æ¢åˆ°CPUæ¨¡å¼...")
                        self.device = "cpu"

                except Exception as e:
                    logging.error(f"âŒ åŠ è½½å¤±è´¥: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(2 * (attempt + 1))
                    else:
                        return False

            return False

    @auto_retry_on_oom(max_retries=3)
    def encode_texts(
        self,
        texts: list[str],
        batch_size: int = 32,
        show_progress: bool = True,
        is_query: bool = False,
    ) -> np.ndarray | None:
        """
        ç¼–ç æ–‡æœ¬ä¸ºåµŒå…¥å‘é‡

        Args:
            texts: å¾…ç¼–ç çš„æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            is_query: æ˜¯å¦ä¸ºæŸ¥è¯¢æ–‡æœ¬ï¼ˆä½¿ç”¨ç‰¹æ®Špromptï¼‰

        Returns:
            np.ndarray: åµŒå…¥å‘é‡çŸ©é˜µï¼Œå¤±è´¥è¿”å›None
        """
        if not self.model:
            logging.error("âŒ æ¨¡å‹æœªåŠ è½½")
            return None

        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        memory_before = get_memory_info()

        # ç¼–ç å‚æ•°
        encode_kwargs = {
            "batch_size": batch_size,
            "show_progress_bar": show_progress,
            "normalize_embeddings": True,
        }

        # å¦‚æœæ˜¯æŸ¥è¯¢ï¼Œä½¿ç”¨query prompt
        if is_query:
            encode_kwargs["prompt_name"] = "query"

        # æ‰§è¡Œç¼–ç 
        embeddings = self.model.encode(texts, **encode_kwargs)

        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        processing_time = time.time() - start_time
        memory_after = get_memory_info()

        metrics = PerformanceMetrics(
            processing_time=processing_time,
            texts_count=len(texts),
            embeddings_shape=embeddings.shape,
            throughput=len(texts) / processing_time,
            memory_before=memory_before,
            memory_after=memory_after,
        )

        self.performance_history.append(metrics)
        logging.info(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡: {metrics}")

        return embeddings

    def compute_similarity(
        self, embeddings1: np.ndarray, embeddings2: np.ndarray
    ) -> np.ndarray:
        """è®¡ç®—åµŒå…¥å‘é‡é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        return cosine_similarity(embeddings1, embeddings2)

    def search(
        self, query: str, documents: list[str], top_k: int = 5
    ) -> list[dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›æœ€ç›¸ä¼¼çš„Kä¸ªç»“æœ

        Returns:
            List[Dict]: åŒ…å«æ–‡æ¡£ã€ç›¸ä¼¼åº¦å’Œæ’åçš„ç»“æœåˆ—è¡¨
        """
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.encode_texts([query], is_query=True, show_progress=False)
        if query_embedding is None:
            return []

        # ç¼–ç æ–‡æ¡£
        doc_embeddings = self.encode_texts(
            documents, show_progress=len(documents) > 100
        )
        if doc_embeddings is None:
            return []

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = self.compute_similarity(query_embedding, doc_embeddings)[0]

        # è·å–top-kç»“æœ
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        results = []
        for rank, idx in enumerate(top_indices, 1):
            results.append(
                {
                    "rank": rank,
                    "document": documents[idx],
                    "similarity": float(similarities[idx]),
                    "index": int(idx),
                }
            )

        return results

    def get_performance_summary(self) -> dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡æ‘˜è¦"""
        if not self.performance_history:
            return {}

        total_texts = sum(m.texts_count for m in self.performance_history)
        total_time = sum(m.processing_time for m in self.performance_history)
        avg_throughput = total_texts / total_time if total_time > 0 else 0

        return {
            "total_operations": len(self.performance_history),
            "total_texts_processed": total_texts,
            "total_processing_time": total_time,
            "average_throughput": avg_throughput,
            "peak_gpu_memory": max(
                m.memory_after.gpu_used for m in self.performance_history
            ),
        }


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# æ¼”ç¤ºå‡½æ•°
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


def demo_basic_usage():
    """åŸºç¡€ä½¿ç”¨æ¼”ç¤º"""
    print("\n" + "=" * 80)
    print("ğŸ¯ åŸºç¡€åµŒå…¥ç”Ÿæˆæ¼”ç¤º")
    print("=" * 80)

    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = Qwen3EmbeddingManager()

    # åŠ è½½æ¨¡å‹
    if not manager.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "è‹¹æœå…¬å¸æ˜¯å…¨çƒæœ€å¤§çš„ç§‘æŠ€å…¬å¸ä¹‹ä¸€",
        "Apple Inc. is one of the world's largest technology companies",
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
        "Artificial intelligence is transforming the world",
        "æ¯”äºšè¿ªæ˜¯ä¸­å›½é¢†å…ˆçš„æ–°èƒ½æºæ±½è½¦åˆ¶é€ å•†",
        "BYD is a leading new energy vehicle manufacturer in China",
    ]

    # ç”ŸæˆåµŒå…¥
    with memory_management("åµŒå…¥ç”Ÿæˆ"):
        embeddings = manager.encode_texts(test_texts, batch_size=32)

        if embeddings is not None:
            print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(test_texts)} ä¸ªæ–‡æœ¬çš„åµŒå…¥")
            print(f"ğŸ“ åµŒå…¥å½¢çŠ¶: {embeddings.shape}")

            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = manager.compute_similarity(embeddings, embeddings)

            print("\nğŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆéƒ¨åˆ†ï¼‰:")
            for i in range(min(3, len(test_texts))):
                print(f"\næ–‡æœ¬ {i + 1}: {test_texts[i][:30]}...")
                for j in range(min(3, len(test_texts))):
                    if i != j:
                        print(f"  ä¸æ–‡æœ¬ {j + 1}: {similarity_matrix[i][j]:.4f}")


def demo_semantic_search():
    """è¯­ä¹‰æœç´¢æ¼”ç¤º"""
    print("\n" + "=" * 80)
    print("ğŸ” è¯­ä¹‰æœç´¢æ¼”ç¤º")
    print("=" * 80)

    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = Qwen3EmbeddingManager()

    if not manager.load_model():
        return

    # æ–‡æ¡£åº“
    documents = [
        "è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°çš„iPhone 15ç³»åˆ—ï¼Œæ­è½½A17 ProèŠ¯ç‰‡",
        "å¾®è½¯æ¨å‡ºäº†æœ€æ–°çš„Surface Proè®¾å¤‡ï¼Œé…å¤‡Intelæœ€æ–°å¤„ç†å™¨",
        "è°·æ­Œå‘å¸ƒäº†Gemini AIæ¨¡å‹ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚",
        "ç‰¹æ–¯æ‹‰å±•ç¤ºäº†æ–°çš„Model 3æ”¹æ¬¾ï¼Œç»­èˆªé‡Œç¨‹å¤§å¹…æå‡",
        "è‹±ä¼Ÿè¾¾å‘å¸ƒäº†RTX 4090 GPUï¼Œä¸“ä¸ºAIå’Œæ¸¸æˆä¼˜åŒ–",
        "æ¯”äºšè¿ªå‘å¸ƒäº†æµ·è±¹è½¦å‹ï¼Œé…å¤‡åˆ€ç‰‡ç”µæ± æŠ€æœ¯",
        "å®å¾·æ—¶ä»£æ¨å‡ºäº†æ–°çš„ç”µæ± æŠ€æœ¯ï¼Œèƒ½é‡å¯†åº¦åˆ›æ–°é«˜",
        "åä¸ºå‘å¸ƒäº†Mate 60ç³»åˆ—æ‰‹æœºï¼Œæ”¯æŒå«æ˜Ÿé€šä¿¡",
    ]

    # æµ‹è¯•æŸ¥è¯¢
    queries = ["è‹¹æœçš„æ–°äº§å“", "AIç›¸å…³çš„æ–°é—»", "ç”µåŠ¨æ±½è½¦æŠ€æœ¯"]

    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        print("-" * 50)

        results = manager.search(query, documents, top_k=3)

        for result in results:
            print(
                f"{result['rank']}. [{result['similarity']:.4f}] {result['document']}"
            )


def demo_batch_performance():
    """æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("âš¡ æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 80)

    manager = Qwen3EmbeddingManager()

    if not manager.load_model():
        return

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_sizes = [50, 100, 200]
    batch_sizes = [16, 32, 64]

    results = []

    for text_count in test_sizes:
        test_texts = [
            f"æµ‹è¯•æ–‡æœ¬ {i}: è¿™æ˜¯ç”¨äºæ€§èƒ½æµ‹è¯•çš„ç¤ºä¾‹æ–‡æœ¬å†…å®¹" for i in range(text_count)
        ]

        print(f"\nğŸ“ æµ‹è¯• {text_count} ä¸ªæ–‡æœ¬:")

        for batch_size in batch_sizes:
            with memory_management(f"Batch-{batch_size}"):
                embeddings = manager.encode_texts(
                    test_texts, batch_size=batch_size, show_progress=False
                )

                if embeddings is not None:
                    # è·å–æœ€æ–°çš„æ€§èƒ½æŒ‡æ ‡
                    latest_metric = manager.performance_history[-1]
                    print(
                        f"  æ‰¹å¤§å° {batch_size:2d}: {latest_metric.throughput:6.1f} texts/ç§’"
                    )

                    results.append(
                        {
                            "text_count": text_count,
                            "batch_size": batch_size,
                            "throughput": latest_metric.throughput,
                        }
                    )

    # æ˜¾ç¤ºæ€§èƒ½æ‘˜è¦
    print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡æ‘˜è¦:")
    summary = manager.get_performance_summary()
    print(f"  æ€»æ“ä½œæ¬¡æ•°: {summary.get('total_operations', 0)}")
    print(f"  æ€»å¤„ç†æ–‡æœ¬: {summary.get('total_texts_processed', 0)}")
    print(f"  å¹³å‡ååé‡: {summary.get('average_throughput', 0):.1f} texts/ç§’")
    print(f"  GPUå†…å­˜å³°å€¼: {summary.get('peak_gpu_memory', 0):.2f}GB")


def demo_dimension_comparison():
    """ä¸åŒåµŒå…¥ç»´åº¦å¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("ğŸ“ åµŒå…¥ç»´åº¦æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)

    dimensions = [1024, 1536, 2048, 2560]
    test_texts = ["æµ‹è¯•æ–‡æœ¬"] * 100

    results = []

    for dim in dimensions:
        print(f"\nğŸ”¢ æµ‹è¯•ç»´åº¦: {dim}")

        # ä¸ºæ¯ä¸ªç»´åº¦åˆ›å»ºæ–°çš„ç®¡ç†å™¨
        manager = Qwen3EmbeddingManager(embedding_dim=dim)

        with memory_management(f"ç»´åº¦-{dim}"):
            if manager.load_model():
                embeddings = manager.encode_texts(test_texts, show_progress=False)

                if embeddings is not None:
                    metric = manager.performance_history[-1]

                    results.append(
                        {
                            "dimension": dim,
                            "throughput": metric.throughput,
                            "memory_used": metric.memory_after.gpu_used,
                            "actual_dim": embeddings.shape[1],
                        }
                    )

                    print(f"  âœ… ååé‡: {metric.throughput:.1f} texts/ç§’")
                    print(f"  ğŸ’¾ GPUå†…å­˜: {metric.memory_after.gpu_used:.2f}GB")
                else:
                    print("  âŒ æµ‹è¯•å¤±è´¥")

            # æ¸…ç†æ¨¡å‹
            del manager

    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    if results:
        print("\nğŸ“Š ç»´åº¦å¯¹æ¯”æ€»ç»“:")
        print(f"{'ç»´åº¦':>6} | {'ååé‡':>12} | {'GPUå†…å­˜':>10}")
        print("-" * 35)
        for r in results:
            print(
                f"{r['dimension']:>6} | {r['throughput']:>10.1f}/s | {r['memory_used']:>8.2f}GB"
            )

        # æ¨èæœ€ä½³ç»´åº¦
        best = max(results, key=lambda x: x["throughput"])
        print(
            f"\nğŸ† æ¨èç»´åº¦: {best['dimension']} (æœ€é«˜ååé‡: {best['throughput']:.1f} texts/ç§’)"
        )


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ä¸»å‡½æ•°
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("â•”" + "â•" * 78 + "â•—")
    print("â•‘" + " " * 20 + "Qwen3-Embedding-4B é«˜çº§ä½¿ç”¨ç¤ºä¾‹" + " " * 26 + "â•‘")
    print("â•š" + "â•" * 78 + "â•")

    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    print("\nğŸ“‹ ç³»ç»Ÿä¿¡æ¯:")
    print(f"  Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(
            f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB"
        )

    # è¿è¡Œæ¼”ç¤º
    demos = [
        ("åŸºç¡€ä½¿ç”¨", demo_basic_usage),
        ("è¯­ä¹‰æœç´¢", demo_semantic_search),
        ("æ‰¹å¤„ç†æ€§èƒ½", demo_batch_performance),
        ("ç»´åº¦å¯¹æ¯”", demo_dimension_comparison),
    ]

    for i, (name, demo_func) in enumerate(demos, 1):
        try:
            print(f"\n\n{'=' * 80}")
            print(f"[{i}/{len(demos)}] {name}")
            print("=" * 80)
            demo_func()

            # æ¼”ç¤ºé—´æ¸…ç†å†…å­˜
            clear_memory()
            time.sleep(1)

        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ æ¼”ç¤ºå‡ºé”™: {e}")
            import traceback

            traceback.print_exc()

    print("\n\n" + "=" * 80)
    print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("=" * 80)


if __name__ == "__main__":
    main()
