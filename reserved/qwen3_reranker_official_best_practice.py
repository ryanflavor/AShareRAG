#!/usr/bin/env python3
"""
Qwen3-Reranker-4B å®˜æ–¹æ–¹æ³•æœ€ä½³å®è·µ
ä½¿ç”¨ç”Ÿæˆå¼æ–¹æ³•è®¡ç®— yes/no æ¦‚ç‡æ¥è¯„åˆ†
æ”¯æŒæ‰¹å¤„ç†ã€OOM æ¢å¤ã€æ€§èƒ½ç›‘æ§ç­‰ç”Ÿäº§çº§åŠŸèƒ½
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
import time
import logging
import sys
import os
import gc
from contextlib import contextmanager
import numpy as np

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# æ•°æ®ç±»å®šä¹‰
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


@dataclass
class RerankerConfig:
    """é‡æ’åºå™¨é…ç½®"""

    model_name: str = "Qwen/Qwen3-Reranker-4B"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    dtype: torch.dtype = torch.float16
    max_length: int = 8192
    batch_size: int = 8
    use_bf16: bool = True
    cache_dir: Optional[str] = None
    log_level: str = "INFO"


@dataclass
class RerankResult:
    """é‡æ’åºç»“æœ"""

    document: str
    score: float
    original_index: int
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""

    total_documents: int
    processing_time: float
    throughput: float
    peak_memory_mb: float
    average_score: float
    score_range: Tuple[float, float]


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ä¸»è¦å®ç°
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


class Qwen3RerankerOfficial:
    """
    Qwen3-Reranker-4B å®˜æ–¹å®ç°
    ä½¿ç”¨ç”Ÿæˆå¼æ–¹æ³•é€šè¿‡ yes/no æ¦‚ç‡è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
    """

    def __init__(self, config: Optional[RerankerConfig] = None):
        self.config = config or RerankerConfig()
        self.logger = self._setup_logger()

        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model()

        # é¢„å®šä¹‰çš„ prompt æ¨¡æ¿ï¼ˆå®˜æ–¹æ ¼å¼ï¼‰
        self.prefix = '<|im_start|>system\nJudge. Note that the answer can only be "yes" or "no".<|im_end|>\n<|im_start|>user\n'
        self.suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"

        # é¢„è®¡ç®— token ids
        self.token_yes = self.tokenizer.convert_tokens_to_ids("yes")
        self.token_no = self.tokenizer.convert_tokens_to_ids("no")

        # æ€§èƒ½ç»Ÿè®¡
        self.total_processed = 0
        self.total_time = 0.0

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logger = logging.getLogger("Qwen3RerankerOfficial")
        logger.setLevel(getattr(logging, self.config.log_level))

        if not logger.handlers:
            handler = logging.StreamHandler(sys.stdout)
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        self.logger.info(f"åŠ è½½æ¨¡å‹: {self.config.model_name}")

        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            cache_dir=self.config.cache_dir,
        )

        # è®¾ç½® padding é…ç½®
        self.tokenizer.padding_side = "left"
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # ç¡®å®šæ•°æ®ç±»å‹
        if self.config.use_bf16 and torch.cuda.is_bf16_supported():
            dtype = torch.bfloat16
            self.logger.info("ä½¿ç”¨ BFloat16 ç²¾åº¦")
        else:
            dtype = self.config.dtype
            self.logger.info(f"ä½¿ç”¨ {dtype} ç²¾åº¦")

        # åŠ è½½æ¨¡å‹ï¼ˆæ³¨æ„ï¼šä½¿ç”¨ CausalLMï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            torch_dtype=dtype,
            device_map="auto" if self.config.device == "cuda" else None,
            trust_remote_code=True,
            cache_dir=self.config.cache_dir,
        )

        self.model.eval()

        # å¦‚æœæ²¡æœ‰ä½¿ç”¨ device_mapï¼Œæ‰‹åŠ¨ç§»åˆ°è®¾å¤‡
        if self.config.device == "cuda" and not hasattr(self.model, "hf_device_map"):
            self.model = self.model.to(self.config.device)

        self.logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
        self._log_memory_usage()

    def _format_input(self, query: str, document: str) -> str:
        """æ ¼å¼åŒ–å•ä¸ªè¾“å…¥"""
        instruction = f'Given a query "{query}", does the following document answer the query? "{document}"'
        return self.prefix + instruction + self.suffix

    def _log_memory_usage(self):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            self.logger.debug(
                f"GPU å†…å­˜: å·²åˆ†é… {allocated:.2f}GB / å·²ä¿ç•™ {reserved:.2f}GB"
            )

    @contextmanager
    def _memory_efficient_mode(self):
        """å†…å­˜é«˜æ•ˆæ¨¡å¼ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        yield
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    def _process_batch(self, query: str, documents: List[str]) -> List[float]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        # æ ¼å¼åŒ–è¾“å…¥
        batch_inputs = [self._format_input(query, doc) for doc in documents]

        # Tokenize
        inputs = self.tokenizer(
            batch_inputs,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=self.config.max_length,
        )

        # ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
        if self.config.device == "cuda":
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits[:, -1, :]  # æœ€åä¸€ä¸ª token çš„ logits

            # è·å– yes å’Œ no çš„ logits
            yes_logits = logits[:, self.token_yes]
            no_logits = logits[:, self.token_no]

            # è®¡ç®— softmax æ¦‚ç‡
            probs = torch.softmax(torch.stack([no_logits, yes_logits], dim=-1), dim=-1)
            scores = probs[:, 1].float().cpu().numpy()  # yes çš„æ¦‚ç‡

        return scores.tolist()

    def rerank(
        self,
        query: str,
        documents: List[str],
        batch_size: Optional[int] = None,
        top_k: Optional[int] = None,
        return_metrics: bool = False,
    ) -> Tuple[List[RerankResult], Optional[PerformanceMetrics]]:
        """
        é‡æ’åºæ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆNone ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
            return_metrics: æ˜¯å¦è¿”å›æ€§èƒ½æŒ‡æ ‡

        Returns:
            (æ’åºåçš„ç»“æœåˆ—è¡¨, æ€§èƒ½æŒ‡æ ‡)
        """
        if not documents:
            return [], None

        batch_size = batch_size or self.config.batch_size
        start_time = time.time()
        peak_memory = 0

        self.logger.info(f"å¼€å§‹é‡æ’åº {len(documents)} ä¸ªæ–‡æ¡£")

        # æ‰¹é‡å¤„ç†
        all_scores = []

        try:
            with self._memory_efficient_mode():
                for i in range(0, len(documents), batch_size):
                    batch_docs = documents[i : i + batch_size]

                    # å¤„ç†æ‰¹æ¬¡
                    try:
                        batch_scores = self._process_batch(query, batch_docs)
                        all_scores.extend(batch_scores)

                        # è®°å½•å†…å­˜ä½¿ç”¨
                        if torch.cuda.is_available():
                            current_memory = torch.cuda.memory_allocated() / 1024**2
                            peak_memory = max(peak_memory, current_memory)

                        self.logger.debug(
                            f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(documents) - 1) // batch_size + 1}"
                        )

                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            self.logger.warning(f"OOM é”™è¯¯ï¼Œå°è¯•å•æ–‡æ¡£å¤„ç†")
                            # æ¸…ç†å†…å­˜
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()

                            # é€ä¸ªå¤„ç†
                            for doc in batch_docs:
                                score = self._process_batch(query, [doc])[0]
                                all_scores.append(score)
                        else:
                            raise

        except Exception as e:
            self.logger.error(f"é‡æ’åºå¤±è´¥: {str(e)}")
            raise

        # åˆ›å»ºç»“æœ
        results = []
        for i, (doc, score) in enumerate(zip(documents, all_scores)):
            results.append(RerankResult(document=doc, score=score, original_index=i))

        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)

        # åº”ç”¨ top_k
        if top_k is not None and top_k < len(results):
            results = results[:top_k]

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        elapsed_time = time.time() - start_time
        self.total_processed += len(documents)
        self.total_time += elapsed_time

        metrics = None
        if return_metrics:
            metrics = PerformanceMetrics(
                total_documents=len(documents),
                processing_time=elapsed_time,
                throughput=len(documents) / elapsed_time,
                peak_memory_mb=peak_memory,
                average_score=np.mean(all_scores),
                score_range=(min(all_scores), max(all_scores)),
            )

            self.logger.info(
                f"å®Œæˆ! å¤„ç†æ—¶é—´: {elapsed_time:.2f}s, "
                f"ååé‡: {metrics.throughput:.1f} docs/s, "
                f"åˆ†æ•°èŒƒå›´: {metrics.score_range[0]:.4f}-{metrics.score_range[1]:.4f}"
            )

        return results, metrics

    def rerank_with_metadata(
        self,
        query: str,
        documents: List[Dict[str, Any]],
        text_key: str = "text",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """
        é‡æ’åºå¸¦å…ƒæ•°æ®çš„æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£å­—å…¸åˆ—è¡¨
            text_key: æ–‡æœ¬å­—æ®µçš„é”®å
            **kwargs: ä¼ é€’ç»™ rerank çš„å…¶ä»–å‚æ•°

        Returns:
            æ’åºåçš„æ–‡æ¡£åˆ—è¡¨ï¼ˆåŒ…å«åŸå§‹å…ƒæ•°æ®å’Œåˆ†æ•°ï¼‰
        """
        # æå–æ–‡æœ¬
        texts = [doc[text_key] for doc in documents]

        # é‡æ’åº
        results, _ = self.rerank(query, texts, **kwargs)

        # ç»„åˆç»“æœ
        ranked_docs = []
        for i, result in enumerate(results):
            doc = documents[result.original_index].copy()
            doc["rerank_score"] = result.score
            doc["rerank_rank"] = i + 1
            ranked_docs.append(doc)

        return ranked_docs

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_documents_processed": self.total_processed,
            "total_processing_time": self.total_time,
            "average_throughput": self.total_processed / self.total_time
            if self.total_time > 0
            else 0,
            "model_name": self.config.model_name,
            "device": self.config.device,
            "batch_size": self.config.batch_size,
        }


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# æµ‹è¯•å’Œç¤ºä¾‹
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    print("ğŸš€ Qwen3-Reranker-4B å®˜æ–¹æ–¹æ³•æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºé…ç½®
    config = RerankerConfig(batch_size=8, use_bf16=True, log_level="INFO")

    # åˆå§‹åŒ–é‡æ’åºå™¨
    reranker = Qwen3RerankerOfficial(config)

    # æµ‹è¯•æ•°æ®
    query = "Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹"
    documents = [
        "Pythonæ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´ä¼˜é›…",
        "Pythonå¹¿æ³›åº”ç”¨äºæ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’ŒWebå¼€å‘",
        "Javaæ˜¯ä¸€ç§å¼ºç±»å‹çš„ç¼–ç¨‹è¯­è¨€",
        "ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œé€‚åˆå¤–å‡ºæ¸¸ç©",
        "æˆ‘å–œæ¬¢åƒè‹¹æœ",
        "Pythonæ‹¥æœ‰ä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“å’Œæ´»è·ƒçš„ç¤¾åŒº",
        "C++æ˜¯ä¸€ç§ç¼–è¯‘å‹è¯­è¨€ï¼Œæ€§èƒ½ä¼˜ç§€",
        "Pythonçš„åŠ¨æ€ç±»å‹ç³»ç»Ÿä½¿å¾—å¼€å‘æ›´åŠ çµæ´»",
    ]

    # æ‰§è¡Œé‡æ’åº
    results, metrics = reranker.rerank(query, documents, top_k=5, return_metrics=True)

    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ† é‡æ’åºç»“æœ (Top 5):")
    print("-" * 60)
    for i, result in enumerate(results):
        doc_preview = (
            result.document[:60] + "..."
            if len(result.document) > 60
            else result.document
        )
        print(f"{i + 1}. [åˆ†æ•°: {result.score:.4f}] {doc_preview}")

    # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
    if metrics:
        print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"- å¤„ç†æ–‡æ¡£æ•°: {metrics.total_documents}")
        print(f"- å¤„ç†æ—¶é—´: {metrics.processing_time:.2f}ç§’")
        print(f"- ååé‡: {metrics.throughput:.1f} docs/ç§’")
        print(f"- å³°å€¼å†…å­˜: {metrics.peak_memory_mb:.1f} MB")
        print(f"- å¹³å‡åˆ†æ•°: {metrics.average_score:.4f}")
        print(
            f"- åˆ†æ•°èŒƒå›´: {metrics.score_range[0]:.4f} - {metrics.score_range[1]:.4f}"
        )

    # æµ‹è¯•å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£
    print("\n\nğŸ“‹ æµ‹è¯•å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£:")
    print("-" * 60)

    docs_with_metadata = [
        {"id": 1, "text": "Pythonæ˜¯é¢å‘å¯¹è±¡çš„ç¼–ç¨‹è¯­è¨€", "source": "wiki"},
        {"id": 2, "text": "ä»Šå¤©è‚¡å¸‚å¤§æ¶¨", "source": "news"},
        {"id": 3, "text": "Pythonåœ¨æ•°æ®ç§‘å­¦é¢†åŸŸåº”ç”¨å¹¿æ³›", "source": "blog"},
    ]

    ranked_docs = reranker.rerank_with_metadata(query, docs_with_metadata)

    for doc in ranked_docs:
        print(
            f"ID: {doc['id']}, åˆ†æ•°: {doc['rerank_score']:.4f}, æ¥æº: {doc['source']}"
        )
        print(f"å†…å®¹: {doc['text']}")
        print()

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = reranker.get_stats()
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"- æ€»å¤„ç†æ–‡æ¡£æ•°: {stats['total_documents_processed']}")
    print(f"- å¹³å‡ååé‡: {stats['average_throughput']:.1f} docs/ç§’")


if __name__ == "__main__":
    main()
