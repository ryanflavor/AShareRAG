# Story 2.1 Query Intent Router - Flowchart and Details

## System Overview

The Query Intent Router is a critical component that analyzes user queries and routes them to appropriate processing flows based on detected intent.

## Flowchart

```mermaid
flowchart TD
    Start([User Query Input]) --> Validate{Query Valid?}
    
    Validate -->|No| Error[Log Error & Return Default]
    Error --> DefaultIntent[Return 'fact_qa' Intent]
    
    Validate -->|Yes| CheckCache{Cache Enabled?}
    
    CheckCache -->|Yes| CacheLookup{Query in Cache?}
    CacheLookup -->|Yes| ReturnCached[Return Cached Result]
    
    CheckCache -->|No| KeywordDetect
    CacheLookup -->|No| KeywordDetect
    
    KeywordDetect[Keyword Detection]
    KeywordDetect --> CheckKeywords{Contains Keywords?<br/>相似/关联/类似/<br/>竞品/上下游}
    
    CheckKeywords -->|Yes| RelationshipIntent[Set Intent:<br/>'relationship_discovery']
    CheckKeywords -->|No| CheckLLM{Use LLM<br/>Classification?}
    
    CheckLLM -->|No| FactQAIntent[Set Intent:<br/>'fact_qa']
    CheckLLM -->|Yes| LLMClassify[LLM Classification<br/>(15-20s)]
    
    LLMClassify --> LLMResult{LLM Confidence<br/>> Threshold?}
    LLMResult -->|Yes| UseLLMIntent[Use LLM Intent]
    LLMResult -->|No| FactQAIntent
    
    RelationshipIntent --> BuildResult
    FactQAIntent --> BuildResult
    UseLLMIntent --> BuildResult
    ReturnCached --> End
    
    BuildResult[Build QueryRouteResult]
    BuildResult --> LogDecision[Log Classification Decision]
    LogDecision --> CacheResult{Cache Enabled?}
    
    CacheResult -->|Yes| StoreCache[Store in Cache]
    StoreCache --> ReturnResult
    CacheResult -->|No| ReturnResult
    
    ReturnResult[Return QueryRouteResult]
    ReturnResult --> End([End])
    
    DefaultIntent --> BuildResult
```

## Component Details

### 1. QueryIntentRouter Class Structure

```python
@dataclass
class QueryRouteResult:
    intent_type: str  # "fact_qa" or "relationship_discovery"
    confidence: float  # 0.0 to 1.0
    hint: Optional[str] = None  # Optional hint for ambiguous queries
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time_ms: float = 0.0

class QueryIntentRouter:
    def __init__(
        self,
        llm_adapter: Optional[LLMAdapter] = None,
        use_llm_fallback: bool = False,
        cache_enabled: bool = False,
        cache_ttl: int = 3600,  # 1 hour default
        performance_monitor: Optional[PerformanceMonitor] = None
    ):
        self.llm_adapter = llm_adapter
        self.use_llm_fallback = use_llm_fallback
        self.cache_enabled = cache_enabled
        self.cache = LRUCache(maxsize=1000) if cache_enabled else None
        self.cache_ttl = cache_ttl
        self.performance_monitor = performance_monitor
        
        # Compile keyword patterns for performance
        self.relationship_keywords = {
            "相似", "类似", "关联", "竞品", "竞争对手",
            "上游", "下游", "上下游", "产业链"
        }
        self.keyword_pattern = re.compile(
            '|'.join(re.escape(kw) for kw in self.relationship_keywords)
        )
```

### 2. Core Methods

#### 2.1 Main Routing Method

```python
def route_query(self, query: str) -> QueryRouteResult:
    """
    Main entry point for query intent classification.
    
    Performance targets:
    - Keyword detection: < 100ms
    - LLM classification: 15-20s (if used)
    """
    start_time = time.time()
    
    # Input validation
    if not self._validate_query(query):
        return self._build_default_result("Invalid query")
    
    # Check cache first
    if self.cache_enabled:
        cached_result = self._get_from_cache(query)
        if cached_result:
            return cached_result
    
    # Primary method: Keyword detection
    keyword_result = self._detect_by_keywords(query)
    if keyword_result.intent_type == "relationship_discovery":
        result = keyword_result
    elif self.use_llm_fallback and self.llm_adapter:
        # Secondary method: LLM classification (optional)
        result = self._classify_with_llm(query)
    else:
        # Default to fact-based Q&A
        result = self._build_fact_qa_result()
    
    # Add processing time
    result.processing_time_ms = (time.time() - start_time) * 1000
    
    # Log and cache
    self._log_classification(query, result)
    if self.cache_enabled:
        self._store_in_cache(query, result)
    
    return result
```

#### 2.2 Keyword Detection (Primary Method)

```python
def _detect_by_keywords(self, query: str) -> QueryRouteResult:
    """
    Fast keyword-based detection - primary classification method.
    Target: < 100ms
    """
    if self.keyword_pattern.search(query):
        return QueryRouteResult(
            intent_type="relationship_discovery",
            confidence=1.0,
            metadata={"method": "keyword_detection"}
        )
    
    # No keywords found
    return QueryRouteResult(
        intent_type="fact_qa",
        confidence=0.8,  # Lower confidence since no explicit signal
        metadata={"method": "no_keywords"}
    )
```

#### 2.3 LLM Classification (Optional Fallback)

```python
def _classify_with_llm(self, query: str) -> QueryRouteResult:
    """
    LLM-based classification for ambiguous cases.
    Expected time: 15-20s
    """
    try:
        prompt = self._build_classification_prompt(query)
        response = self.llm_adapter.generate(
            prompt=prompt,
            max_tokens=100,
            temperature=0.1  # Low temperature for consistency
        )
        
        # Parse LLM response
        intent, confidence = self._parse_llm_response(response)
        
        return QueryRouteResult(
            intent_type=intent,
            confidence=confidence,
            metadata={
                "method": "llm_classification",
                "llm_response": response
            }
        )
    except Exception as e:
        logger.error(f"LLM classification failed: {e}")
        # Fallback to fact_qa on error
        return self._build_fact_qa_result()
```

### 3. Integration Points

#### 3.1 Upstream Integration (FastAPI)

```python
# In /src/server/main.py
@app.post("/api/query")
async def process_query(request: QueryRequest):
    # Initialize router
    router = QueryIntentRouter(
        llm_adapter=get_llm_adapter(),
        use_llm_fallback=settings.USE_LLM_INTENT_FALLBACK,
        cache_enabled=settings.ENABLE_INTENT_CACHE
    )
    
    # Route query
    route_result = router.route_query(request.query)
    
    # Route to appropriate handler
    if route_result.intent_type == "relationship_discovery":
        return await handle_relationship_query(request, route_result)
    else:
        return await handle_fact_query(request, route_result)
```

#### 3.2 Downstream Integration

```python
# Relationship Discovery Flow
async def handle_relationship_query(request, route_result):
    hybrid_retriever = HybridRetriever(
        vector_store=get_vector_store(),
        graph_store=get_graph_store()
    )
    results = await hybrid_retriever.retrieve(
        query=request.query,
        intent_metadata=route_result.metadata
    )
    return results

# Fact-based Q&A Flow  
async def handle_fact_query(request, route_result):
    vector_retriever = VectorRetriever(
        vector_store=get_vector_store()
    )
    results = await vector_retriever.retrieve(
        query=request.query,
        k=request.top_k
    )
    return results
```

### 4. Configuration

#### 4.1 Prompts Configuration (/config/prompts.yaml)

```yaml
query_intent_classification:
  system: |
    You are a query intent classifier for a financial knowledge base system.
    Classify queries into one of two categories:
    1. "fact_qa" - Queries seeking specific facts, data, or information
    2. "relationship_discovery" - Queries exploring relationships between entities
    
    Consider these signals for relationship discovery:
    - Keywords: 相似 (similar), 关联 (related), 类似 (alike), 竞品 (competitor), 上下游 (upstream/downstream)
    - Comparison requests between entities
    - Supply chain or industry relationship queries
    
    Output format: {"intent": "<intent_type>", "confidence": <0.0-1.0>}
  
  user: |
    Query: {query}
    
    Classify this query's intent.
```

#### 4.2 Settings Configuration

```python
# In /config/settings.py
class IntentRouterSettings(BaseSettings):
    # Performance settings
    KEYWORD_DETECTION_TIMEOUT_MS: int = 100
    LLM_CLASSIFICATION_TIMEOUT_S: int = 30
    
    # Feature flags
    USE_LLM_INTENT_FALLBACK: bool = False
    ENABLE_INTENT_CACHE: bool = False
    
    # Cache settings
    INTENT_CACHE_TTL_SECONDS: int = 3600
    INTENT_CACHE_MAX_SIZE: int = 1000
    
    # LLM settings
    LLM_CONFIDENCE_THRESHOLD: float = 0.7
```

### 5. Performance Monitoring

```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "keyword_detection": [],
            "llm_classification": [],
            "cache_hits": 0,
            "cache_misses": 0
        }
    
    def record_latency(self, method: str, latency_ms: float):
        self.metrics[method].append(latency_ms)
    
    def get_percentiles(self, method: str):
        latencies = self.metrics[method]
        if not latencies:
            return {}
        
        return {
            "p50": np.percentile(latencies, 50),
            "p95": np.percentile(latencies, 95),
            "p99": np.percentile(latencies, 99),
            "count": len(latencies)
        }
```

### 6. Testing Strategy

#### 6.1 Unit Tests Structure

```python
# /tests/unit/test_query_intent_router.py
class TestQueryIntentRouter:
    def test_keyword_detection_chinese(self):
        """Test all Chinese relationship keywords"""
        router = QueryIntentRouter()
        test_cases = [
            ("找出与阿里巴巴相似的公司", "relationship_discovery"),
            ("分析腾讯的竞品", "relationship_discovery"),
            ("查看供应链上下游关系", "relationship_discovery"),
            ("阿里巴巴的营收是多少", "fact_qa")
        ]
        
    def test_performance_keyword_detection(self, benchmark):
        """Ensure keyword detection < 100ms"""
        router = QueryIntentRouter()
        result = benchmark(router._detect_by_keywords, "测试查询")
        assert benchmark.stats['mean'] < 0.1  # 100ms
```

#### 6.2 Integration Tests

```python
# /tests/integration/test_query_intent_routing.py
@pytest.mark.integration
async def test_full_routing_flow():
    """Test complete routing with downstream components"""
    router = QueryIntentRouter(
        llm_adapter=MockLLMAdapter(),
        use_llm_fallback=True
    )
    
    # Mock downstream components
    mock_vector_retriever = Mock(spec=VectorRetriever)
    mock_hybrid_retriever = Mock(spec=HybridRetriever)
    
    # Test routing decisions
    result = router.route_query("找出与腾讯相似的公司")
    assert result.intent_type == "relationship_discovery"
```

### 7. Error Handling

```python
def _validate_query(self, query: str) -> bool:
    """Validate input query"""
    if not query or not isinstance(query, str):
        return False
    
    # Check length
    if len(query) > 1000:
        logger.warning(f"Query too long: {len(query)} chars")
        return False
    
    # Basic sanitization
    if any(char in query for char in ['<script>', 'DROP TABLE']):
        logger.warning("Potential injection attempt detected")
        return False
    
    return True
```

## Summary

This implementation provides:
1. **Fast keyword detection** as the primary method (< 100ms)
2. **Optional LLM fallback** for ambiguous cases (15-20s acceptable)
3. **Optional caching** for performance optimization
4. **Robust error handling** and input validation
5. **Comprehensive logging** and monitoring
6. **Clear integration points** with upstream/downstream components

The design prioritizes speed and reliability while accepting realistic performance constraints for LLM operations.