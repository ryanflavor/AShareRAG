# Story 1.1: Project Setup and Data Ingestion

## Status: Approved

## Story

**As a** system developer,  
**I want** to set up the project foundation with proper configuration and implement data ingestion from corpus.json,  
**so that** we have a working development environment and can load company data for further processing

## Acceptance Criteria

1. Project is properly initialized with Python 3.10 and uv package manager
2. All required dependencies from tech stack are installed and configured
3. Project follows the defined directory structure from architecture
4. Configuration system is implemented using Pydantic Settings to load environment variables
5. Data ingestion module can successfully read and parse corpus.json file
6. Each document's text field is treated as a single chunk (no splitting) as per FR1
7. Unit tests exist for all implemented functionality following TDD approach
8. Code passes ruff format checks

## Tasks / Subtasks

- [ ] Task 1: Initialize project structure and dependencies (AC: 1, 2, 3)
  - [ ] Initialize Python project with uv and create pyproject.toml
  - [ ] Install core dependencies: HippoRAG ~2.0.0a4, python-igraph ~0.11.9, LanceDB ~0.24.0
  - [ ] Install ML dependencies: Transformers ~4.42.0, Sentence-Transformers ~3.0.1
  - [ ] Install development dependencies: pytest ~8.2.2, ruff
  - [ ] Create directory structure as per architecture spec
  - [ ] Create .env.example template file
  - [ ] Move corpus.json from reserved/corpus.json to data/corpus.json

- [ ] Task 2: Implement configuration system (AC: 4)
  - [ ] Create config/settings.py using Pydantic Settings
  - [ ] Define configuration schema for API keys, model paths, and system settings
  - [ ] Implement environment variable loading
  - [ ] Create config/prompts.yaml structure (empty for now)

- [ ] Task 3: Implement data ingestion component (AC: 5, 6)
  - [ ] Create src/components/__init__.py
  - [ ] Create src/components/data_ingestor.py
  - [ ] Implement corpus.json reader that returns documents with metadata
  - [ ] Ensure each document's text is treated as a single chunk (no splitting)
  - [ ] Add proper error handling for missing or malformed data

- [ ] Task 4: Write comprehensive tests following TDD (AC: 7)
  - [ ] Create tests/conftest.py with pytest configuration
  - [ ] Create tests/unit/__init__.py
  - [ ] Write tests/unit/test_data_ingestor.py
  - [ ] Test successful loading of valid corpus.json
  - [ ] Test error handling for missing file
  - [ ] Test error handling for malformed JSON
  - [ ] Test that text fields are not split

- [ ] Task 5: Setup code quality checks (AC: 8)
  - [ ] Configure ruff in pyproject.toml
  - [ ] Run ruff format on all code
  - [ ] Ensure all tests pass

## Dev Notes

### Technology Stack (Source: architecture/1-架构概述与核心原则.md)
- **Python**: ~3.10 (primary language)
- **Package Manager**: uv ~0.7.19 (10-100x faster than pip)
- **Core Framework**: HippoRAG ~2.0.0a4 (for graph RAG capabilities)
- **Graph Library**: python-igraph ~0.11.9 (for PPR computation)
- **Vector DB**: LanceDB ~0.24.0 (file-based, no deployment needed)
- **Testing**: Pytest ~8.2.2

### Project Structure (Source: architecture/3-统一项目结构-source-tree.md)
```
a_share_rag_project/
├── pyproject.toml         # Project metadata and uv config
├── requirements.txt       # Managed by uv
├── uv.lock               # Lock file for dependencies
├── .env.example          # Environment variables template
├── config/
│   ├── prompts.yaml      # Prompt templates (empty initially)
│   └── settings.py       # Pydantic Settings implementation
├── data/
│   └── corpus.json       # Input data file
├── output/               # Will store generated assets
│   ├── graph/           # Future: igraph files
│   └── vector_store/    # Future: LanceDB files
├── src/
│   ├── components/
│   │   ├── __init__.py
│   │   └── data_ingestor.py  # First component to implement
│   └── pipeline.py       # Future: main pipeline script
└── tests/
    ├── conftest.py       # Pytest configuration
    └── unit/
        ├── __init__.py
        └── test_data_ingestor.py
```

### Epic 1 Context (Source: PRD Epic 1 & FR1-FR6)
This story is the first of Epic 1: 基础数据管道与索引构建, which includes:
- FR1: Document Processing (this story)
- FR2: Named Entity Recognition (future story)
- FR3: Relation Extraction (future story) 
- FR4: Knowledge Graph Construction (future story)
- FR5: Text Embedding (future story)
- FR6: Vector Indexing (future story)

### Data Processing Requirements (Source: PRD FR1)
- System must read `corpus.json` format
- Each document's complete `text` field is one chunk (no splitting)
- This differs from typical text splitting - we treat entire documents as chunks

### Corpus.json Structure Example
```json
[
  {
    "title": "公司简称",      // Company short name
    "text": "# 公司全称\n\n## 公司简称\n...",  // Full company info in markdown
    "idx": 0                // Unique index number
  },
  // ... more company documents
]
```
Each document contains:
- **title**: Company name/identifier
- **text**: Complete company information in markdown format including business segments, products, services
- **idx**: Unique numeric index

### Development Standards (Source: architecture/4-ai开发工作流与交付标准.md)
- **Mandatory TDD**: Write failing test first, then minimal implementation
- All tests must pass 100% before marking complete
- Code must pass `ruff format` checks
- Fill in Dev Agent Record section when complete

### Design Principles (Source: architecture/1-架构概述与核心原则.md)
- Use HippoRAG as library without modifying its source
- All customizations via adapter classes
- Maintain clear separation between graph and vector storage

### Testing

Testing Standards from Architecture:
- Use pytest framework (~8.2.2)
- Tests go in `tests/unit/` for unit tests
- Follow strict TDD: test first, then code
- 100% test pass rate required
- Test file naming: `test_[module_name].py`

## Change Log

| Date | Version | Description | Author |
| :--- | :------ | :---------- | :----- |
| 2025-01-05 | 1.0 | Initial story creation | SM (Bob) |

## Dev Agent Record

### Agent Model Used: {{Agent Model Name/Version}}

### Debug Log References

### Completion Notes List

### File List

## QA Results